{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1LwlZeD31o1ODWwHNnw4sXFjWGFGmU6UO",
      "authorship_tag": "ABX9TyOCGORVXDOLodKyc6/qP/lL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaehyeopKim/cs231n_stanford/blob/Lecture10/min_char_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MDDpoa6j-jf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1904b306-161c-439c-c230-44511e288e00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 6831 characters, 67 unique.\n",
            "----\n",
            " qUm1N03AuriC plkjhVIl1yú.34’”OiTJrCPxghA.\n",
            "U’dG\n",
            "“J2’3Xmi7XunvexAydy”liTqsbDDofT49Nh—OI3XSl40SYJsIH,xa.5hruBC4CnYTl“”,I.cgqa”FsoeUXjWC6jjúB,\n",
            "A—Y6Ny6M4-j2S9n7dds-6w c8“egnF”SDYJGW’9’lBT.hútnj h 6E7taYn”o \n",
            "----\n",
            "iter 0, loss: 105.117310\n",
            "----\n",
            " r.\n",
            "Bh to thies to ao lhe ked tourtumesguaoxit dos ptot pre mbat rtU.\n",
            "voug ros ardos dU thw t imuner-pprippr d me\n",
            "Or horse ngou voveonpther mere sf trapo.Oun kerete notdone Aren Tdd tor parou, pnar Me  \n",
            "----\n",
            "iter 1000, loss: 84.708438\n",
            "----\n",
            " torininto s tied dr wad Un wiwelinof an shingnivefe’s thi thed ctuclneila kee Axiale maldelerinds iminq thinzsintories.\n",
            "Mnr icin-ingo theradd lonthiold dnlenisithifktidesd rithe nlies in farif-leranfo \n",
            "----\n",
            "iter 2000, loss: 66.205517\n",
            "----\n",
            " tupcpultrinse Wulindsro ss nn thons, ficent the Io to hexicl rtiand countr whtr hgpoude duops bounderuse trre, is nn thros hungres Chine.T.Crint riess trainse poute tor Wtrosn the Uning’s turdent ad r \n",
            "----\n",
            "iter 3000, loss: 55.653465\n",
            "----\n",
            " do theiffs likesistrhame trat aping cosratit dartla theiffsy gricl as The tayid, coutpeics trated 36 bapindly — thons. The toade Mexica-ith thawerstrorlated peacidt countries whe desero,s, lhet asplhe \n",
            "----\n",
            "iter 4000, loss: 49.289456\n",
            "----\n",
            "  what bobl wim ovesided thede tariffs in r7unt int to l1wis China\n",
            "Aplutc. Thing comld ffreb serthak to parei heve heatfor enmind coplased porled a tond cakts of ciagse txade an the wion Inos and to th \n",
            "----\n",
            "iter 5000, loss: 45.538808\n",
            "----\n",
            " t tertevest topiciocsunaso dumly hntirwith any comlicpevtrilly,eithed exponcen20WToonarids rivehowseraly inStarils the frhin veat tarilrtrols on Indtrale trupesing lly in gorcampuchising countrouce co \n",
            "----\n",
            "iter 6000, loss: 43.104160\n",
            "----\n",
            " ng ind-serleld woule tariff — svind walese tori9, on s af pountries werizad expoole ware to raing countries on to whely bovenopeelecw alpevis on 3lrinet murd tore to raike ond frle to louts ald woile’ \n",
            "----\n",
            "iter 7000, loss: 40.909619\n",
            "----\n",
            " incont of larieg raillornd, and hal Thot on thame, Shamlareariof to the torle goods heving mied bi-perimplaqe coA dary Berthumerithid Mr. Trumpoind..\n",
            "CrisesA s on China.\n",
            "Mexico bon mocre bore of of ra \n",
            "----\n",
            "iter 8000, loss: 39.440618\n",
            "----\n",
            " dobend the 2, romuhes nounh bissing cound couldude lould could canan alliqeargartarifgrcy agres eraco gricim tieg o00thrallon biudeatith the Gerils harsunvembrer lool, bas bare Chinanicistwe orlithe B \n",
            "----\n",
            "iter 9000, loss: 38.110712\n",
            "----\n",
            " picle the reziplen foundulese Coinese to th t weiceng could atr ametpein, cec. Dre thicf — lere, s. Inonien oselase, bald ened levemeng Thake rules, indoad wotceil, expealede, s al ond irgond Chinese  \n",
            "----\n",
            "iter 10000, loss: 36.731090\n",
            "----\n",
            " me trade ond chons the to licwing and regrs, bisilavenvego Stargasing laly Mr. Trumprie a cean Bekeicistorsargacembehiff Chinese Bptss coartrghind oa strank tradieg cann gos in oremply ramenopteriseri \n",
            "----\n",
            "iter 11000, loss: 35.603285\n",
            "----\n",
            " se freminet ant leveves in 20s aclen Bromineretse hore ted lunde tarinfevele Thiip, porew rhe navard 2goin 2018teated Pnes oranly hgre, Juconer Mr. Trudesemumqries, rivo Agler appeisitr lemecaesuzane  \n",
            "----\n",
            "iter 12000, loss: 34.780016\n",
            "----\n",
            " gree the NorpperiSt iball, bealey bicsies ers berse sint Chinese ing could d its tend chuce aplesift. coon .O. Merocle a cementate, wes Suans. Treas, vere feregopicist and wits here merpethed the duy  \n",
            "----\n",
            "iter 13000, loss: 33.825063\n",
            "----\n",
            " ing purther the Acinlutt ablhing the for tre coreire Tikd Ther Mr. Trats for countrads tart otedingocsaret in fir theiss they Mexico gre to sizes countries China bess and thane crays rad suopiontrien  \n",
            "----\n",
            "iter 14000, loss: 33.045286\n",
            "----\n",
            " as in Bream aad Mexico me to n ond findly that a veruhed to that jauqeruse al with Chinen , otemend and ricts asted avies ind So the United Staten mbont the ruCt far for fon to dispurd the United Stat \n",
            "----\n",
            "iter 15000, loss: 32.542801\n",
            "----\n",
            " redurializa trade feralise tariffs goids dy with liger incound ties aut w a — countries trade 11.\n",
            "“I incom.T.O. W.T.O.he comedidild ruleror alle stimp’s marked te siatseackis acred ive bention appling \n",
            "----\n",
            "iter 16000, loss: 31.838311\n",
            "----\n",
            " nithing courtrics it trade the worcluadutegring brease bools. Dowifstiben Statea inclan andy accase of tort atobehing. The Treabe fore ware troded trait ambant orevent bpetse averturiffA ory upy an th \n",
            "----\n",
            "iter 17000, loss: 31.181639\n",
            "----\n",
            " atser exs rizasidt the to lurked cronder trade from Buipl. Inmesintraek the United States ar Ampeised tariffs Ameriart Trade Agope al pore of lont date for W.T.O. Trame derblle more Chinen murisose ic \n",
            "----\n",
            "iter 18000, loss: 30.687982\n",
            "----\n",
            " s offirgot and is orizing cowed the Unitexido hebl the coend avers but freede ward. I.-Mr.TWatrainime cavelound” ceral Tred Ssate, mon SATtuccound” ad bule weijieg ing ly fraen Beijids.\n",
            "The durity wie \n",
            "----\n",
            "iter 19000, loss: 30.191490\n",
            "----\n",
            "  poreg Sxadire comh fot is from by with maring the partelith of r lepmintray is wheper.\n",
            "Ast ast traden moode, it thawked that a soress erplinelawievely rement comlat Vo Mexicth Chinese dema Sored uspr \n",
            "----\n",
            "iter 20000, loss: 29.511739\n",
            "----\n",
            " Woriet malyiale wirting expelea aly rusual 3Wamelasid the United Stavelouded Chinese tariff coule Statighanizeverom, developpedeecom.T.O. Trade Wasdidind a sale tarilW.Atrade tariff goodt itplly Jed e \n",
            "----\n",
            "iter 21000, loss: 29.147526\n",
            "----\n",
            " ustrease Burinct s ot at crump has wien mie mont of care Beinike so exhep igs start limatingpy in 39lieicWor suith bakees, to fgom China, frabs der Stain, countries rezrugemidist maca Nrrek wizmleked  \n",
            "----\n",
            "iter 22000, loss: 28.813339\n",
            "----\n",
            " fs tariffs abpedes in Mexico.\n",
            "Und ammreat a fid Chinese wirc Chinese ithon, mulks and ficsing partise ruldsuntries in Mr. Trest byuces the ports.\n",
            "Chines orcand the Genen Mexico in thet miadien trade f \n",
            "----\n",
            "iter 23000, loss: 28.508898\n",
            "----\n",
            "  Stane Nnoing acly unot appainevercave werom, to caveaning lild egensperduntracr.\n",
            "China ig the wyelaialiclull his lould thas bunding that 31W. urean 19 torin’s average, rean a to acfar pardidirtargect \n",
            "----\n",
            "iter 24000, loss: 28.235589\n",
            "----\n",
            " ergenl, 19liss enolesting woulduwel that 34 tore mike Oustroncruck mentse as avesuinme the W.T.O., and goode countraes impalith 3990 sr un theisedw acleveratn sizer the trade ouruis the UniBthil, rome \n",
            "----\n",
            "iter 25000, loss: 27.727987\n",
            "----\n",
            " ly ffo Fre pred ouce Tued to chinstine.\n",
            "Mr. Trade, mourtry starto the trade wore the United States all wialiof Mr. Trump in Chines orecund that is expood fot dedy by it tradudesidercendutembuntroess c \n",
            "----\n",
            "iter 26000, loss: 27.524330\n",
            "----\n",
            " o ridet an Mexico nrienly good, fot in is in trade wargevely wfteridaring that Was ragegg isent mandimp haten po d the l atively furane ap lipling the 20vert a se cland stadies orgoshitherestijill of  \n",
            "----\n",
            "iter 27000, loss: 27.262962\n",
            "----\n",
            " iling to ce that wall of — on tarifnt in to “been tariffs other inconer and comadt theyed from Mexiclewol goon oncoucto re1 Sery Trump ist aviegavent bes in trat on, ragali’s ducont ible agreement ram \n",
            "----\n",
            "iter 28000, loss: 26.829992\n",
            "----\n",
            "  te to stant tariffs of the Gunona rayse the 2lits is precearged ceatergect Amereves wes thas oinaveragembunt parked Thade wien the Norcing to dhonge these to ships. happede deverted perteduces iamgab \n",
            "----\n",
            "iter 29000, loss: 26.434989\n",
            "----\n",
            " ly durind could ig the Gliby bunqin, chest has butited pent China’s tradessing theinas binding percent. Indunckeveretsiont is and Trait and — avpetsiggerigasialle by when itcaged Mexiclund loken the i \n",
            "----\n",
            "iter 30000, loss: 26.212799\n",
            "----\n",
            " an oor fot birylat sherigs bany on maven countriesing countries — abal Merecount upe crayt egrensint 34 levaldere igntrizpout wat whet tariffs.\n",
            "Chine, motidts on the Americe trade Amo andend ally egow \n",
            "----\n",
            "iter 31000, loss: 25.861725\n",
            "----\n",
            " he United States because itsenc.T.O. Trump’t toods Mr. Thane doclise buspere with into trade surpent medieg torge that munts of gordar ats to of theingenn al bepehmerting came ss exports bluke.\n",
            "Thet a \n",
            "----\n",
            "iter 32000, loss: 25.656937\n",
            "----\n",
            " nd develrade becauclard ance tariff — prapt ofs. Treidhen porre thits. Indusing of the isriealizasieg cimadied the wievind to the U.-savelly forivitinment hind ationt.\n",
            "Mr. Wadest itperule, basee, Mr.  \n",
            "----\n",
            "iter 33000, loss: 25.626040\n",
            "----\n",
            "  trade such Tram suspict erad and 2buneveridest averagared asmuning itsue. Whend devers steca itcliagreh fre to of Mexico 1lete arods. Amoth ravely op tariff vewir the world States iffothien tomind fi \n",
            "----\n",
            "iter 34000, loss: 25.345671\n",
            "----\n",
            " ked atnobs from China nnink Mr. Trumpire theis accese Mexico. Agiclaysides in tariffs and to ce percent with Mexico. Sowowh Beijings mizated they China has wares is thes to theil un theichon coont mas \n",
            "----\n",
            "iter 35000, loss: 25.076795\n",
            "----\n",
            " s trade, W.Tracse ith the dar China, st unienso trade for ig the Unijint boude Mexico wat like Mexico ban 1995 tariff st highemitr atsegant jter draiq ant r fot trrde,.\n",
            "7 ctucked Wastion theisedess to \n",
            "----\n",
            "iter 36000, loss: 24.823519\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
        "BSD License\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "# data I/O\n",
        "data = open('input.txt', 'r').read() # should be simple plain text file\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars) #2110, 60\n",
        "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# hyperparameters\n",
        "hidden_size = 100 # size of hidden layer of neurons\n",
        "seq_length = 25 # number of steps to unroll the RNN for\n",
        "learning_rate = 1e-1\n",
        "\n",
        "# model parameters\n",
        "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
        "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
        "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
        "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
        "by = np.zeros((vocab_size, 1)) # output bias\n",
        "\n",
        "def lossFun(inputs, targets, hprev):\n",
        "  \"\"\"\n",
        "  inputs,targets are both list of integers.\n",
        "  hprev is Hx1 array of initial hidden state\n",
        "  returns the loss, gradients on model parameters, and last hidden state\n",
        "  \"\"\"\n",
        "  xs, hs, ys, ps = {}, {}, {}, {}\n",
        "  hs[-1] = np.copy(hprev)\n",
        "  loss = 0\n",
        "  # forward pass\n",
        "  for t in range(len(inputs)):\n",
        "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
        "    xs[t][inputs[t]] = 1\n",
        "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
        "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
        "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
        "    loss += -np.log(ps[t][targets[t]]) # softmax (cross-entropy loss)\n",
        "  # backward pass: compute gradients going backwards\n",
        "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "  dhnext = np.zeros_like(hs[0])\n",
        "  for t in reversed(range(len(inputs))):\n",
        "    dy = np.copy(ps[t])\n",
        "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
        "    dWhy += np.dot(dy, hs[t].T)\n",
        "    dby += dy\n",
        "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
        "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
        "    dbh += dhraw\n",
        "    dWxh += np.dot(dhraw, xs[t].T)\n",
        "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "    dhnext = np.dot(Whh.T, dhraw)\n",
        "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
        "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
        "\n",
        "def sample(h, seed_ix, n):\n",
        "  \"\"\"\n",
        "  sample a sequence of integers from the model\n",
        "  h is memory state, seed_ix is seed letter for first time step\n",
        "  \"\"\"\n",
        "  x = np.zeros((vocab_size, 1))\n",
        "  x[seed_ix] = 1\n",
        "  ixes = []\n",
        "  for t in range(n):\n",
        "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
        "    y = np.dot(Why, h) + by\n",
        "    p = np.exp(y) / np.sum(np.exp(y))\n",
        "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    x[ix] = 1\n",
        "    ixes.append(ix)\n",
        "  return ixes\n",
        "\n",
        "n, p = 0, 0\n",
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
        "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
        "while True:\n",
        "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
        "  if p+seq_length+1 >= len(data) or n == 0:\n",
        "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "    p = 0 # go from start of data\n",
        "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
        "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "\n",
        "  # sample from the model now and then\n",
        "  if n % 1000 == 0:\n",
        "    sample_ix = sample(hprev, inputs[0], 200)\n",
        "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "    print('----\\n %s \\n----' % (txt, ))\n",
        "\n",
        "  # forward seq_length characters through the net and fetch gradient\n",
        "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "  if n % 1000 == 0: print('iter %d, loss: %f' % (n, np.sum(smooth_loss))) # print progress\n",
        "\n",
        "  # perform parameter update with Adagrad\n",
        "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
        "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
        "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "    mem += dparam * dparam\n",
        "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
        "\n",
        "  p += seq_length # move data pointer\n",
        "  n += 1 # iteration counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"2012abcd@snu.ac.kr\"\n",
        "!git config --global user.name \"DaehyeopKim\"\n",
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "EHOvPS1EIfVB"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}